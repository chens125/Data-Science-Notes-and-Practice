{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Decision Tree\n"," This task creates a decision tree that can predict the survival of passengers on the Titanic\n"," ![image.png](attachment:image.png)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mFPFeEu69axo"},"outputs":[],"source":["# importing libraries\n","import numpy as np\n","import pandas as pd\n","\n","from sklearn.datasets import load_wine\n","\n","\n","# Import Decision Tree Classifier\n","from sklearn.tree import DecisionTreeClassifier, plot_tree\n","from sklearn import tree\n","from sklearn.ensemble import BaggingClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.ensemble import AdaBoostClassifier\n","\n","# Splitting data into training and testing set\n","from sklearn.model_selection import train_test_split\n","\n","from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n","from sklearn.metrics import classification_report\n","\n","# from sklearn.metrics import f1_score, precision_score, recall_score\n","\n","# for visualisation\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.tree import export_graphviz\n","from subprocess import call\n","\n","# import os"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e9C-68Ij9ayD","outputId":"cb08f206-100d-4a4c-f87d-13f2aec390cc"},"outputs":[],"source":["titanic_df = pd.read_csv(\"titanic.csv\")\n","titanic_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CBV7Mx939ayO","outputId":"4e34ffef-0da5-49e5-d52c-379240000dc2"},"outputs":[],"source":["titanic_df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EhNnIWET9ayS"},"outputs":[],"source":["# You can also drop whichever other columns you'd like here\n","titanic_df.drop([\"PassengerId\", \"Cabin\", \"Name\", \"Ticket\"], axis=1, inplace=True)"]},{"cell_type":"markdown","metadata":{"id":"zFZ50hDg9aye"},"source":["### One-Hot Encoding\n","One-hot encoding is a technique used to ensure that categorical variables are better represented in the machine. Let's take a look at the \"Sex\" column"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zxl62Q0-9ay1","outputId":"1b2e62ad-3707-424d-fc21-26d413fc9221"},"outputs":[],"source":["titanic_df[\"Sex\"].unique()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hMLpI7IP9azC","outputId":"8973bd4a-7ecd-4abf-edda-ef096ce958d7"},"outputs":[],"source":["titanic_df = pd.get_dummies(titanic_df, prefix=\"Sex\", columns=[\"Sex\"])\n","titanic_df = pd.get_dummies(titanic_df, prefix=\"Embarked\", columns=[\"Embarked\"])\n","titanic_df.head()"]},{"cell_type":"markdown","metadata":{},"source":["Features and target"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X = titanic_df.drop(columns=[\"Survived\"])\n","y = titanic_df[\"Survived\"]\n","X.shape"]},{"cell_type":"markdown","metadata":{},"source":["Train Test Split"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# random state\n","r = 42\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=r)"]},{"cell_type":"markdown","metadata":{},"source":["### 1. Decision Tree & Bagged trees\n","\n","Check train and test accuracy\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["names = [\"Decision Tree\", \"Bagged Tree\", \"Random Forest\"]\n","\n","classifier = [\n","    DecisionTreeClassifier(random_state=r),\n","    BaggingClassifier(\n","        estimator=DecisionTreeClassifier(random_state=r),\n","        random_state=r,\n","    ),\n","    RandomForestClassifier(random_state=r),\n","]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["data = []\n","for name, log in zip(names, classifier):\n","    log.fit(X_train, y_train)\n","    y_prediction = log.predict(X_test)\n","\n","    accuracy = log.score(X_test, y_test)\n","    data.append([name, accuracy])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from tabulate import tabulate\n","\n","print(\n","    tabulate(\n","        data,\n","        headers=[\"Model\", \"Accuracy\"],\n","        tablefmt=\"fancy_outline\",\n","        colalign=(\"center\",),\n","    )\n",")"]},{"cell_type":"markdown","metadata":{},"source":["### 2. Feature Importance"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["rf = RandomForestClassifier(random_state=r)\n","rf.fit(X_train, y_train)\n","\n","# Finding the important features using the built-in Gini importance\n","\n","# Get numerical feature importances\n","feature_names = X.columns\n","importances = rf.feature_importances_\n","\n","# Dataframe with features and importances, # Sort the feature importances by most important first\n","feature_imp_df = pd.DataFrame(\n","    {\"Feature\": feature_names, \"Importance\": importances}\n",").sort_values(\"Importance\", ascending=False)\n","\n","feature_imp_df"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Creating a seaborn bar plot\n","plt.figure(figsize=(7, 7))\n","sns.barplot(\n","    x=feature_imp_df[\"Importance\"], y=feature_imp_df[\"Feature\"], data=feature_imp_df\n",")\n","plt.title(\"Feature Importance\")\n","plt.xlabel(\"Importance score\")\n","plt.ylabel(\"Features\")"]},{"cell_type":"markdown","metadata":{},"source":["As shown in the above graph, \"Fare\" feature contributes the most when predicting survival rate. "]},{"cell_type":"markdown","metadata":{},"source":["### 3. Hyperparameter Tuning"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define a base Random Forest model\n","rf = RandomForestClassifier(random_state=r)\n","\n","# Hyperparameter tuning for Random Forest using GridSearchCV and fit the data.\n","from sklearn.model_selection import GridSearchCV\n","\n","params = {\n","    \"max_depth\": [2, 3, 5, 10, 20],\n","    \"max_features\": [1, 2, 3, 6, 10],\n","    \"n_estimators\": [10, 25, 30, 50, 100, 200],\n","}\n","\n","# Instantiate the grid search model\n","grid_search = GridSearchCV(\n","    estimator=rf, param_grid=params, cv=3, n_jobs=-1, verbose=1, scoring=\"r2\"\n",")\n","\n","grid_search.fit(X_train, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["grid_search.best_estimator_"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(grid_search.best_params_)\n","best_depth = grid_search.best_params_[\"max_depth\"]\n","best_estimators = grid_search.best_params_[\"n_estimators\"]\n","best_features = grid_search.best_params_[\"max_features\"]"]},{"cell_type":"markdown","metadata":{},"source":["### 4. Comparing Models (second time)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["new_log = RandomForestClassifier(\n","    max_depth=best_depth,\n","    max_features=best_features,\n","    n_estimators=best_estimators,\n","    random_state=r,\n",")\n","new_log.fit(X_train, y_train)\n","y_prediction = new_log.predict(X_test)\n","\n","new_accuracy = new_log.score(X_test, y_test)\n","data.append([\"Random Forest with pre-pruning\", new_accuracy])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from tabulate import tabulate\n","\n","print(\n","    tabulate(\n","        data,\n","        headers=[\"Model\", \"Accuracy\"],\n","        tablefmt=\"fancy_outline\",\n","        colalign=(\"center\",),\n","    )\n",")"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"cvML","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"1904059d3876957b542b45423f2a26c6c4608f5e11cc75420e543fa77f94b066"}}},"nbformat":4,"nbformat_minor":0}
